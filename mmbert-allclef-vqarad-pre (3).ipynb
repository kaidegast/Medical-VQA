{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Utils","metadata":{}},{"cell_type":"code","source":"!pip install pytorch_lightning\n!pip install transformers\n!pip install pretrainedmodels","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-10-15T14:30:00.461942Z","iopub.execute_input":"2021-10-15T14:30:00.462688Z","iopub.status.idle":"2021-10-15T14:30:28.101174Z","shell.execute_reply.started":"2021-10-15T14:30:00.462555Z","shell.execute_reply":"2021-10-15T14:30:28.100164Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport numpy as np\nimport pandas as pd\nimport random\nimport math\nimport cv2\n\nimport torch\nfrom torchvision import transforms, models\nfrom torch.cuda.amp import GradScaler\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader, Dataset\nimport torch.optim as optim\nimport torch.optim.lr_scheduler as lr_scheduler\nimport torch.nn.functional as F\nfrom transformers import BertTokenizer, BertModel\nfrom nltk.translate.bleu_score import sentence_bleu\nfrom tqdm import tqdm\nfrom PIL import Image\nfrom random import choice\nimport matplotlib.pyplot as plt\n\nimport pretrainedmodels","metadata":{"execution":{"iopub.status.busy":"2021-10-15T14:30:28.104837Z","iopub.execute_input":"2021-10-15T14:30:28.105135Z","iopub.status.idle":"2021-10-15T14:30:32.43803Z","shell.execute_reply.started":"2021-10-15T14:30:28.105104Z","shell.execute_reply":"2021-10-15T14:30:32.437336Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def seed_everything(seed):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    random.seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = True","metadata":{"execution":{"iopub.status.busy":"2021-10-15T14:30:32.439189Z","iopub.execute_input":"2021-10-15T14:30:32.439714Z","iopub.status.idle":"2021-10-15T14:30:32.445227Z","shell.execute_reply.started":"2021-10-15T14:30:32.439671Z","shell.execute_reply":"2021-10-15T14:30:32.44445Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# get data","metadata":{}},{"cell_type":"code","source":"def make_df(file_path):\n    paths = os.listdir(file_path)\n    \n    df_list = []\n    \n    for p in paths:\n        df = pd.read_csv(os.path.join(file_path, p), sep='|', names = ['img_id', 'question', 'answer'])\n        df['category'] = p.split('_')[1]\n        df['mode'] = p.split('_')[2][:-4]\n        df_list.append(df)\n    \n    return pd.concat(df_list)","metadata":{"execution":{"iopub.status.busy":"2021-10-15T14:30:32.446337Z","iopub.execute_input":"2021-10-15T14:30:32.447077Z","iopub.status.idle":"2021-10-15T14:30:32.459337Z","shell.execute_reply.started":"2021-10-15T14:30:32.447038Z","shell.execute_reply":"2021-10-15T14:30:32.458251Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def load_all_data(args, remove = None):\n    \n    #### 2019 ####\n    clef2019train_path = '../input/clef2019/clef2019/ImageClef-2019-VQA-Med-Training'\n    clef2019valid_path = '../input/clef2019/clef2019/ImageClef-2019-VQA-Med-Validation'\n    clef2019test_path = '../input/clef2019/clef2019/ImageClef-2019-VQA-Med-Test'\n    \n    traindf2019 = pd.read_csv(os.path.join(clef2019train_path, 'traindf.csv'))\n    print(\"traindf2019: \", len(traindf2019))\n    valdf2019 = pd.read_csv(os.path.join(clef2019valid_path, 'valdf.csv'))\n    print(\"valdf2019: \", len(valdf2019))\n    testdf2019 = pd.read_csv(os.path.join(clef2019test_path, 'testdf.csv'))\n    print(\"testdf2019: \", len(testdf2019))\n    \n    if remove is not None:\n        traindf2019 = traindf2019[~traindf2019['img_id'].isin(remove)].reset_index(drop=True)\n\n    traindf2019['img_id'] = traindf2019['img_id'].apply(lambda x: os.path.join(clef2019train_path, 'Train_images', x + '.jpg'))\n    valdf2019['img_id'] = valdf2019['img_id'].apply(lambda x: os.path.join(clef2019valid_path, 'Val_images', x + '.jpg'))\n    testdf2019['img_id'] = testdf2019['img_id'].apply(lambda x: os.path.join(clef2019test_path, 'Test_images', x + '.jpg'))\n    # testdf2019['img_id'] = testdf2019['img_id'].apply(lambda x: os.path.join(args.data_dir2019, x + '.jpg'))\n\n    traindf2019['category'] = traindf2019['category'].str.lower()\n    valdf2019['category'] = valdf2019['category'].str.lower()\n    testdf2019['category'] = testdf2019['category'].str.lower()\n\n    traindf2019['answer'] = traindf2019['answer'].str.lower()\n    valdf2019['answer'] = valdf2019['answer'].str.lower()\n    testdf2019['answer'] = testdf2019['answer'].str.lower()\n\n    traindf2019 = traindf2019.sample(frac = args.train_pct)\n    valdf2019 = valdf2019.sample(frac = args.valid_pct)\n    testdf2019 = testdf2019.sample(frac = args.test_pct)\n    \n    #### 2020 ####\n    clef2020train_path = '../input/clef2020/clef2020/VQA-Med-2020-Task1-VQAnswering-TrainVal-Sets/VQAMed2020-VQAnswering-TrainingSet'\n    clef2020valid_path = '../input/clef2020/clef2020/VQA-Med-2020-Task1-VQAnswering-TrainVal-Sets/VQAMed2020-VQAnswering-ValidationSet'\n    \n    traindf2020 = pd.read_csv(os.path.join(clef2020train_path, 'clef2020_train_category.csv'))\n    valdf2020 = pd.read_csv(os.path.join(clef2020valid_path, 'clef2020_valid_category.csv'))\n\n    traindf2020['img_id'] = traindf2020['img_id'].apply(lambda x: os.path.join(clef2020train_path, 'VQAnswering_2020_Train_images', x + '.jpg'))\n    valdf2020['img_id'] = valdf2020['img_id'].apply(lambda x: os.path.join(clef2020valid_path, 'VQAnswering_2020_Val_images', x + '.jpg'))\n\n    traindf2020['category'] = traindf2020['category'].str.lower()\n    valdf2020['category'] = valdf2020['category'].str.lower()\n\n    traindf2020['answer'] = traindf2020['answer'].str.lower()\n    valdf2020['answer'] = valdf2020['answer'].str.lower()\n\n    traindf2020 = traindf2020.sample(frac = args.train_pct)\n    valdf2020 = valdf2020.sample(frac = args.valid_pct)\n    \n    #### 2018 ####\n    clef2018train_path = '../input/clef2018/clef2018/VQAMed2018Train/VQAMed2018Train'\n    clef2018valid_path = '../input/clef2018/clef2018/VQAMed2018Valid/VQAMed2018Valid'\n    \n    traindf2018 = pd.read_csv(os.path.join(clef2018train_path, 'clef2018_train_category.csv'))\n    valdf2018 = pd.read_csv(os.path.join(clef2018valid_path, 'clef2018_valid_category.csv'))\n\n    traindf2018['img_id'] = traindf2018['img_id'].apply(lambda x: os.path.join(clef2018train_path, 'VQAMed2018Train-images', x + '.jpg'))\n    valdf2018['img_id'] = valdf2018['img_id'].apply(lambda x: os.path.join(clef2018valid_path, 'VQAMed2018Valid-images', x + '.jpg'))\n\n    traindf2018['category'] = traindf2018['category'].str.lower()\n    valdf2018['category'] = valdf2018['category'].str.lower()\n\n    traindf2018['answer'] = traindf2018['answer'].str.lower()\n    valdf2018['answer'] = valdf2018['answer'].str.lower()\n\n    traindf2018 = traindf2018.sample(frac = args.train_pct)\n    valdf2018 = valdf2018.sample(frac = args.valid_pct)\n    \n    #### VQARAD ####\n    vqaradtrain_path='../input/vqarad'\n    traindf_vqarad = pd.read_csv(os.path.join(vqaradtrain_path, 'vqa_rad.csv'))\n\n    traindf_vqarad['img_id'] = traindf_vqarad['img_id'].apply(lambda x: os.path.join(vqaradtrain_path, 'VQA_RAD Image Folder', x + '.jpg'))\n    \n    traindf_vqarad.head()\n    \n    #### union ####\n    \n    data_frames_train = [traindf2018, traindf2019, traindf2020, traindf_vqarad]\n    data_frames_valid = [valdf2018, valdf2019, valdf2020]\n    \n    union_train = pd.concat(data_frames_train, ignore_index=True)\n    union_valid = pd.concat(data_frames_valid, ignore_index=True)\n    \n    return union_train, union_valid, testdf2019","metadata":{"execution":{"iopub.status.busy":"2021-10-15T14:31:58.229582Z","iopub.execute_input":"2021-10-15T14:31:58.22991Z","iopub.status.idle":"2021-10-15T14:31:58.258796Z","shell.execute_reply.started":"2021-10-15T14:31:58.229881Z","shell.execute_reply":"2021-10-15T14:31:58.258154Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## utils methods","metadata":{}},{"cell_type":"code","source":"def gelu(x):\n    return x * 0.5 * (1.0 + torch.erf(x / math.sqrt(2.0)))","metadata":{"execution":{"iopub.status.busy":"2021-10-15T14:30:32.489577Z","iopub.execute_input":"2021-10-15T14:30:32.490262Z","iopub.status.idle":"2021-10-15T14:30:32.502547Z","shell.execute_reply.started":"2021-10-15T14:30:32.490222Z","shell.execute_reply":"2021-10-15T14:30:32.50144Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def encode_text(caption,tokenizer, args):\n    part1 = [0 for _ in range(5)]\n    #get token ids and remove [CLS] and [SEP] token id\n    part2 = tokenizer.encode(caption)[1:-1]\n\n    tokens = [tokenizer.cls_token_id] + part1 + [tokenizer.sep_token_id] + part2[:args.max_position_embeddings-8] + [tokenizer.sep_token_id]\n    segment_ids = [0]*(len(part1)+2) + [1]*(len(part2[:args.max_position_embeddings-8])+1)\n    input_mask = [1]*len(tokens)\n    n_pad = args.max_position_embeddings - len(tokens)\n    tokens.extend([0]*n_pad)\n    segment_ids.extend([0]*n_pad)\n    input_mask.extend([0]*n_pad)\n\n    \n    return tokens, segment_ids, input_mask","metadata":{"execution":{"iopub.status.busy":"2021-10-15T14:30:32.50448Z","iopub.execute_input":"2021-10-15T14:30:32.50471Z","iopub.status.idle":"2021-10-15T14:30:32.51607Z","shell.execute_reply.started":"2021-10-15T14:30:32.504685Z","shell.execute_reply":"2021-10-15T14:30:32.515018Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def onehot(size, target):\n    vec = torch.zeros(size, dtype=torch.float32)\n    vec[target] = 1.\n    return vec","metadata":{"execution":{"iopub.status.busy":"2021-10-15T14:30:32.517684Z","iopub.execute_input":"2021-10-15T14:30:32.518216Z","iopub.status.idle":"2021-10-15T14:30:32.531412Z","shell.execute_reply.started":"2021-10-15T14:30:32.518154Z","shell.execute_reply":"2021-10-15T14:30:32.530514Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class LabelSmoothing(nn.Module):\n    def __init__(self, smoothing = 0.1):\n        super(LabelSmoothing, self).__init__()\n        self.confidence = 1.0 - smoothing\n        self.smoothing = smoothing\n\n    def forward(self, x, target):\n        if self.training:\n            x = x.float()\n            target = target.float()\n            logprobs = torch.nn.functional.log_softmax(x, dim = -1)\n\n            nll_loss = -logprobs * target\n            nll_loss = nll_loss.sum(-1)\n    \n            smooth_loss = -logprobs.mean(dim=-1)\n\n            loss = self.confidence * nll_loss + self.smoothing * smooth_loss\n\n            return loss.mean()\n        else:\n            return torch.nn.functional.cross_entropy(x, target)","metadata":{"execution":{"iopub.status.busy":"2021-10-15T14:30:32.532535Z","iopub.execute_input":"2021-10-15T14:30:32.532752Z","iopub.status.idle":"2021-10-15T14:30:32.542314Z","shell.execute_reply.started":"2021-10-15T14:30:32.532727Z","shell.execute_reply":"2021-10-15T14:30:32.541112Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Classes","metadata":{}},{"cell_type":"code","source":"class VQAMed(Dataset):\n    def __init__(self, df, imgsize, tfm, args, mode = 'train'):\n        self.df = df\n        self.tfm = tfm\n        self.size = imgsize\n        self.args = args\n        self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n        self.mode = mode\n\n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, idx):\n        path = self.df.loc[idx,'img_id']\n        question = self.df.loc[idx, 'question']\n \n        answer = self.df.loc[idx, 'answer']\n\n        if self.mode == 'eval':\n            tok_ques = self.tokenizer.tokenize(question)\n\n        if self.args.smoothing:\n            answer = onehot(self.args.num_classes, answer)\n\n        img = cv2.imread(path)\n  \n\n        if self.tfm:\n            img = self.tfm(img)\n            \n        tokens, segment_ids, input_mask= encode_text(question, self.tokenizer, self.args)\n\n\n        return img, torch.tensor(tokens, dtype = torch.long), torch.tensor(segment_ids, dtype = torch.long), torch.tensor(input_mask, dtype = torch.long), torch.tensor(answer, dtype = torch.long), path","metadata":{"execution":{"iopub.status.busy":"2021-10-15T14:30:32.544028Z","iopub.execute_input":"2021-10-15T14:30:32.544305Z","iopub.status.idle":"2021-10-15T14:30:32.555658Z","shell.execute_reply.started":"2021-10-15T14:30:32.544279Z","shell.execute_reply":"2021-10-15T14:30:32.554691Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Model_Keyword(nn.Module):\n    def __init__(self, num_classes):\n        super(Model_Keyword, self).__init__()\n        self.model = pretrainedmodels.__dict__['se_resnext50_32x4d'](num_classes=1000, pretrained='imagenet')\n        last_in = self.model.last_linear.in_features\n        self.model.last_linear = nn.Identity()\n        self.embed = nn.Embedding(3, last_in)\n        self.last_layer = nn.Linear(2 * last_in, num_classes)\n\n    def forward(self, img, keyword):\n\n        img_feat = self.model(img)\n        key_feat = self.embed(keyword)\n\n        feat = torch.cat([img_feat, key_feat], -1)\n\n        logits = self.last_layer(feat)\n\n        return logits","metadata":{"execution":{"iopub.status.busy":"2021-10-15T14:30:32.557354Z","iopub.execute_input":"2021-10-15T14:30:32.55825Z","iopub.status.idle":"2021-10-15T14:30:32.571182Z","shell.execute_reply.started":"2021-10-15T14:30:32.558181Z","shell.execute_reply":"2021-10-15T14:30:32.570362Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def calculate_bleu_score(preds,targets, idx2ans):\n  bleu_per_answer = np.asarray([sentence_bleu([idx2ans[target].split()],idx2ans[pred].split(), weights = [1]) for pred,target in zip(preds,targets)])\n  return np.mean(bleu_per_answer)","metadata":{"execution":{"iopub.status.busy":"2021-10-15T14:30:32.572476Z","iopub.execute_input":"2021-10-15T14:30:32.572708Z","iopub.status.idle":"2021-10-15T14:30:32.581675Z","shell.execute_reply.started":"2021-10-15T14:30:32.572683Z","shell.execute_reply":"2021-10-15T14:30:32.580888Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Embeddings(nn.Module):\n    def __init__(self, args):\n        super(Embeddings, self).__init__()\n        self.word_embeddings = nn.Embedding(args.vocab_size, 128, padding_idx=0)\n        self.word_embeddings_2 = nn.Linear(128, args.hidden_size, bias=False)\n        self.position_embeddings = nn.Embedding(args.max_position_embeddings, args.hidden_size)\n        self.type_embeddings = nn.Embedding(3, args.hidden_size)\n        self.LayerNorm = nn.LayerNorm(args.hidden_size, eps=1e-12)\n        self.dropout = nn.Dropout(args.hidden_dropout_prob)\n        self.len = args.max_position_embeddings\n    def forward(self, input_ids, segment_ids, position_ids=None):\n        if position_ids is None:\n            if torch.cuda.is_available():\n                position_ids = torch.arange(self.len, dtype=torch.long).cuda()\n            else:\n                position_ids = torch.arange(self.len, dtype=torch.long)\n            position_ids = position_ids.unsqueeze(0).expand_as(input_ids)\n        words_embeddings = self.word_embeddings(input_ids)\n        words_embeddings = self.word_embeddings_2(words_embeddings)\n        position_embeddings = self.position_embeddings(position_ids)\n        token_type_embeddings = self.type_embeddings(segment_ids)\n        embeddings = words_embeddings + position_embeddings + token_type_embeddings\n        embeddings = self.LayerNorm(embeddings)\n        embeddings = self.dropout(embeddings)\n\n        return embeddings","metadata":{"execution":{"iopub.status.busy":"2021-10-15T14:30:32.583036Z","iopub.execute_input":"2021-10-15T14:30:32.583312Z","iopub.status.idle":"2021-10-15T14:30:32.595347Z","shell.execute_reply.started":"2021-10-15T14:30:32.583277Z","shell.execute_reply":"2021-10-15T14:30:32.594116Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Transfer(nn.Module):\n    def __init__(self,args):\n        super(Transfer, self).__init__()\n\n        self.args = args\n        self.num_vis = args.num_vis\n        self.model = models.resnet152(pretrained=True)\n        # for p in self.parameters():\n        #     p.requires_grad=False\n\n        if self.num_vis == 5:\n            self.relu = nn.ReLU()\n            self.conv2 = nn.Conv2d(2048, args.hidden_size, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            self.gap2 = nn.AdaptiveAvgPool2d((1,1))\n            self.conv3 = nn.Conv2d(1024, args.hidden_size, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            self.gap3 = nn.AdaptiveAvgPool2d((1,1))\n            self.conv4 = nn.Conv2d(512, args.hidden_size, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            self.gap4 = nn.AdaptiveAvgPool2d((1,1))\n            self.conv5 = nn.Conv2d(256, args.hidden_size, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            self.gap5 = nn.AdaptiveAvgPool2d((1,1))\n            self.conv7 = nn.Conv2d(64, args.hidden_size, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            self.gap7 = nn.AdaptiveAvgPool2d((1,1))\n\n        elif self.num_vis == 3:\n            self.relu = nn.ReLU()\n            self.conv2 = nn.Conv2d(2048, args.hidden_size, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            self.gap2 = nn.AdaptiveAvgPool2d((1,1))\n            self.conv3 = nn.Conv2d(1024, args.hidden_size, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            self.gap3 = nn.AdaptiveAvgPool2d((1,1))\n            self.conv4 = nn.Conv2d(512, args.hidden_size, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            self.gap4 = nn.AdaptiveAvgPool2d((1,1))\n\n        else:\n            self.relu = nn.ReLU()\n            self.conv2 = nn.Conv2d(2048, args.hidden_size, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            self.gap2 = nn.AdaptiveAvgPool2d((1,1))            \n            \n    def forward(self, img):\n\n        if self.num_vis == 5: \n            modules2 = list(self.model.children())[:-2]\n            fix2 = nn.Sequential(*modules2)\n            inter_2 = self.conv2(fix2(img))\n            v_2 = self.gap2(self.relu(inter_2)).view(-1,self.args.hidden_size)\n            modules3 = list(self.model.children())[:-3]\n            fix3 = nn.Sequential(*modules3)\n            inter_3 = self.conv3(fix3(img))\n            v_3 = self.gap3(self.relu(inter_3)).view(-1,self.args.hidden_size)\n            modules4 = list(self.model.children())[:-4]\n            fix4 = nn.Sequential(*modules4)\n            inter_4 = self.conv4(fix4(img))\n            v_4 = self.gap4(self.relu(inter_4)).view(-1,self.args.hidden_size)\n            modules5 = list(self.model.children())[:-5]\n            fix5 = nn.Sequential(*modules5)\n            inter_5 = self.conv5(fix5(img))\n            v_5 = self.gap5(self.relu(inter_5)).view(-1,self.args.hidden_size)\n            modules7 = list(self.model.children())[:-7]\n            fix7 = nn.Sequential(*modules7)\n            inter_7 = self.conv7(fix7(img))\n            v_7 = self.gap7(self.relu(inter_7)).view(-1,self.args.hidden_size)\n\n            return v_2, v_3, v_4, v_5, v_7, [inter_2.mean(1), inter_3.mean(1), inter_4.mean(1), inter_5.mean(1), inter_7.mean(1)]\n\n        if self.num_vis == 3: \n            modules2 = list(self.model.children())[:-2]\n            fix2 = nn.Sequential(*modules2)\n            inter_2 = self.conv2(fix2(img))\n            v_2 = self.gap2(self.relu(inter_2)).view(-1,self.args.hidden_size)\n            modules3 = list(self.model.children())[:-3]\n            fix3 = nn.Sequential(*modules3)\n            inter_3 = self.conv3(fix3(img))\n            v_3 = self.gap3(self.relu(inter_3)).view(-1,self.args.hidden_size)\n            modules4 = list(self.model.children())[:-4]\n            fix4 = nn.Sequential(*modules4)\n            inter_4 = self.conv4(fix4(img))\n            v_4 = self.gap4(self.relu(inter_4)).view(-1,self.args.hidden_size)\n\n            return v_2, v_3, v_4, [inter_2.mean(1), inter_3.mean(1), inter_4.mean(1)]\n\n        else:\n            modules2 = list(self.model.children())[:-2]\n            fix2 = nn.Sequential(*modules2)\n            inter_2 = self.conv2(fix2(img))\n            v_2 = self.gap2(self.relu(inter_2)).view(-1,self.args.hidden_size)    \n            \n            return v_2, [inter_2.mean(1)]  ","metadata":{"execution":{"iopub.status.busy":"2021-10-15T14:30:32.599238Z","iopub.execute_input":"2021-10-15T14:30:32.599488Z","iopub.status.idle":"2021-10-15T14:30:32.635259Z","shell.execute_reply.started":"2021-10-15T14:30:32.599461Z","shell.execute_reply":"2021-10-15T14:30:32.634179Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class MultiHeadedSelfAttention(nn.Module):\n    def __init__(self,args):\n        super(MultiHeadedSelfAttention,self).__init__()\n        self.proj_q = nn.Linear(args.hidden_size, args.hidden_size)\n        self.proj_k = nn.Linear(args.hidden_size, args.hidden_size)\n        self.proj_v = nn.Linear(args.hidden_size, args.hidden_size)\n        self.drop = nn.Dropout(args.hidden_dropout_prob)\n        self.scores = None\n        self.n_heads = args.heads\n    def forward(self, x, mask):\n        q, k, v = self.proj_q(x), self.proj_k(x), self.proj_v(x)\n        q, k, v = (self.split_last(x, (self.n_heads, -1)).transpose(1, 2) for x in [q, k, v])\n        scores = q @ k.transpose(-2, -1) / np.sqrt(k.size(-1))\n        if mask is not None:\n            mask = mask[:, None, None, :].float()\n            scores -= 10000.0 * (1.0 - mask)\n        scores = self.drop(F.softmax(scores, dim=-1))\n        h = (scores @ v).transpose(1, 2).contiguous()\n        h = self.merge_last(h, 2)\n        self.scores = scores\n        return h, scores\n    def split_last(self, x, shape):\n        shape = list(shape)\n        assert shape.count(-1) <= 1  \n        if -1 in shape:\n            shape[shape.index(-1)] = int(x.size(-1) / -np.prod(shape))\n        return x.view(*x.size()[:-1], *shape)\n    def merge_last(self, x, n_dims):\n        s = x.size()\n        assert n_dims > 1 and n_dims < len(s)\n        return x.view(*s[:-n_dims], -1)","metadata":{"execution":{"iopub.status.busy":"2021-10-15T14:30:32.636713Z","iopub.execute_input":"2021-10-15T14:30:32.636949Z","iopub.status.idle":"2021-10-15T14:30:32.652972Z","shell.execute_reply.started":"2021-10-15T14:30:32.636918Z","shell.execute_reply":"2021-10-15T14:30:32.652106Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class PositionWiseFeedForward(nn.Module):\n    def __init__(self,args):\n        super(PositionWiseFeedForward,self).__init__()\n        self.fc1 = nn.Linear(args.hidden_size, args.hidden_size*4)\n        self.fc2 = nn.Linear(args.hidden_size*4, args.hidden_size)\n    def forward(self, x):\n        return self.fc2(gelu(self.fc1(x)))","metadata":{"execution":{"iopub.status.busy":"2021-10-15T14:30:32.654411Z","iopub.execute_input":"2021-10-15T14:30:32.654734Z","iopub.status.idle":"2021-10-15T14:30:32.667916Z","shell.execute_reply.started":"2021-10-15T14:30:32.654703Z","shell.execute_reply":"2021-10-15T14:30:32.666939Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class BertLayer(nn.Module):\n    def __init__(self,args, share='all', norm='pre'):\n        super(BertLayer, self).__init__()\n        self.share = share\n        self.norm_pos = norm\n        self.norm1 = nn.LayerNorm(args.hidden_size, eps=1e-12)\n        self.norm2 = nn.LayerNorm(args.hidden_size, eps=1e-12)\n        self.drop1 = nn.Dropout(args.hidden_dropout_prob)\n        self.drop2 = nn.Dropout(args.hidden_dropout_prob)\n        if self.share == 'ffn':\n            self.attention = nn.ModuleList([MultiHeadedSelfAttention(args) for _ in range(args.n_layers)])\n            self.proj = nn.ModuleList([nn.Linear(args.hidden_size, args.hidden_size) for _ in range(args.n_layers)])\n            self.feedforward = PositionWiseFeedForward(args)\n        elif self.share == 'att':\n            self.attention = MultiHeadedSelfAttention(args)\n            self.proj = nn.Linear(args.hidden_size, args.hidden_size)\n            self.feedforward = nn.ModuleList([PositionWiseFeedForward(args) for _ in range(args.n_layers)])\n        elif self.share == 'all':\n            self.attention = MultiHeadedSelfAttention(args)\n            self.proj = nn.Linear(args.hidden_size, args.hidden_size)\n            self.feedforward = PositionWiseFeedForward(args)\n        elif self.share == 'none':\n            self.attention = nn.ModuleList([MultiHeadedSelfAttention(args) for _ in range(args.n_layers)])\n            self.proj = nn.ModuleList([nn.Linear(args.hidden_size, args.hidden_size) for _ in range(args.n_layers)])\n            self.feedforward = nn.ModuleList([PositionWiseFeedForward(args) for _ in range(args.n_layers)])\n    def forward(self, hidden_states, attention_mask, layer_num):\n        if self.norm_pos == 'pre':\n            if isinstance(self.attention, nn.ModuleList):\n                attn_output, attn_scores = self.attention[layer_num](self.norm1(hidden_states), attention_mask)\n                h = self.proj[layer_num](attn_output)\n            else:\n                h = self.proj(self.attention(self.norm1(hidden_states), attention_mask))\n            out = hidden_states + self.drop1(h)\n            if isinstance(self.feedforward, nn.ModuleList):\n                h = self.feedforward[layer_num](self.norm1(out))\n            else:\n                h = self.feedforward(self.norm1(out))\n            out = out + self.drop2(h)\n        if self.norm_pos == 'post':\n            if isinstance(self.attention, nn.ModuleList):\n                h = self.proj[layer_num](self.attention[layer_num](hidden_states, attention_mask))\n            else:\n                h = self.proj(self.attention(hidden_states, attention_mask))\n            out = self.norm1(hidden_states + self.drop1(h))\n            if isinstance(self.feedforward, nn.ModuleList):\n                h = self.feedforward[layer_num](out)\n            else:\n                h = self.feedforward(out)\n            out = self.norm2(out + self.drop2(h))\n        return out, attn_scores","metadata":{"execution":{"iopub.status.busy":"2021-10-15T14:30:32.669125Z","iopub.execute_input":"2021-10-15T14:30:32.669352Z","iopub.status.idle":"2021-10-15T14:30:32.691561Z","shell.execute_reply.started":"2021-10-15T14:30:32.669325Z","shell.execute_reply":"2021-10-15T14:30:32.690496Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Transformer(nn.Module):\n    def __init__(self, args):\n        super(Transformer,self).__init__()\n        base_model = BertModel.from_pretrained('bert-base-uncased')\n        bert_model = nn.Sequential(*list(base_model.children())[0:])\n        self.bert_embedding = bert_model[0]\n        # self.embed = Embeddings(args)\n        self.num_vis = args.num_vis\n        self.trans = Transfer(args)\n        self.blocks = BertLayer(args,share='none', norm='pre')\n        self.n_layers = args.n_layers\n        \n    def forward(self, img, input_ids, token_type_ids, mask):\n\n        if self.num_vis==5:\n            #print(\"img.shape: \" ,img.shape)\n            v_2, v_3, v_4, v_5, v_7, intermediate = self.trans(img)\n        elif self.num_vis==3:\n            v_2, v_3, v_4, intermediate = self.trans(img)\n        else:\n            v_2, intermediate = self.trans(img)\n        # h = self.embed(input_ids, token_type_ids)\n        h = self.bert_embedding(input_ids=input_ids, token_type_ids=token_type_ids, position_ids=None)\n        #print(\"h.size: \" ,h.shape)\n        #print(\"v_2.size: \" ,v_2.shape)\n        #print(\"v_3.size: \" ,v_3.shape)\n        #print(\"v_4.size: \" ,v_4.shape)\n        #print(\"v_5.size: \" ,v_5.shape)\n        #print(\"v_7.size: \" ,v_7.shape)\n        if self.num_vis == 5:\n            for i in range(len(h)):\n                h[i][1] = v_2[i]\n            for i in range(len(h)):\n                h[i][2] = v_3[i]\n            for i in range(len(h)):\n                h[i][3] = v_4[i]\n            for i in range(len(h)):\n                h[i][4] = v_5[i]\n            for i in range(len(h)):\n                h[i][5] = v_7[i]\n\n        elif self.num_vis == 3:\n            for i in range(len(h)):\n                h[i][1] = v_2[i]\n            for i in range(len(h)):\n                h[i][2] = v_3[i]\n            for i in range(len(h)):\n                h[i][3] = v_4[i]\n\n        else:\n            for i in range(len(h)):\n                h[i][1] = v_2[i]\n\n\n        hidden_states = []\n        all_attn_scores = []\n        for i in range(self.n_layers):\n            h, attn_scores = self.blocks(h, mask, i)\n            hidden_states.append(h)\n            all_attn_scores.append(attn_scores)\n\n        return torch.stack(hidden_states, 0), torch.stack(all_attn_scores, 0), intermediate","metadata":{"execution":{"iopub.status.busy":"2021-10-15T14:30:32.692832Z","iopub.execute_input":"2021-10-15T14:30:32.693087Z","iopub.status.idle":"2021-10-15T14:30:32.711418Z","shell.execute_reply.started":"2021-10-15T14:30:32.693062Z","shell.execute_reply":"2021-10-15T14:30:32.710284Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Model(nn.Module):\n    def __init__(self,args):\n        super(Model,self).__init__()\n        self.args = args\n        self.transformer = Transformer(args)\n        self.fc1 = nn.Linear(args.hidden_size, args.hidden_size)\n        self.activ1 = nn.Tanh()\n        self.classifier = nn.Sequential(nn.Linear(args.hidden_size, args.hidden_size),\n                                        nn.LayerNorm(args.hidden_size, eps=1e-12, elementwise_affine=True),\n                                        nn.Linear(args.hidden_size, args.vocab_size))\n    def forward(self, img, input_ids, segment_ids, input_mask):\n        h, attn_scores, intermediate = self.transformer(img, input_ids, segment_ids, input_mask)\n        pooled_h = self.activ1(self.fc1(h[-1].mean(1)))\n        logits = self.classifier(pooled_h)\n        return logits, attn_scores, intermediate","metadata":{"execution":{"iopub.status.busy":"2021-10-15T14:30:32.712622Z","iopub.execute_input":"2021-10-15T14:30:32.712918Z","iopub.status.idle":"2021-10-15T14:30:32.725748Z","shell.execute_reply.started":"2021-10-15T14:30:32.712836Z","shell.execute_reply":"2021-10-15T14:30:32.725066Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train_one_epoch(loader, model, optimizer, criterion, device, scaler, args, idx2ans):\n\n    model.train()\n    train_loss = []\n    IMGIDS = []\n    PREDS = []\n    TARGETS = []\n    bar = tqdm(loader, leave = False)\n    for (img, question_token,segment_ids,attention_mask,target, imgid) in bar:\n        \n        img, question_token,segment_ids,attention_mask,target = img.to(device), question_token.to(device), segment_ids.to(device), attention_mask.to(device), target.to(device)\n        question_token = question_token.squeeze(1)\n        attention_mask = attention_mask.squeeze(1)\n        loss_func = criterion\n        optimizer.zero_grad()\n\n        if args.mixed_precision:\n            with torch.cuda.amp.autocast(): \n                logits, _, _ = model(img, question_token, segment_ids, attention_mask)\n                loss = loss_func(logits, target)\n        else:\n            logits, _, _ = model(img, question_token, segment_ids, attention_mask)\n            loss = loss_func(logits, target)\n\n        if args.mixed_precision:\n            scaler.scale(loss)\n            loss.backward()\n\n            if args.clip:\n                nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n\n            scaler.step(optimizer)\n            scaler.update()\n        else:\n            loss.backward()\n\n            if args.clip:\n                nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n                \n            optimizer.step()\n\n        if args.smoothing:\n            TARGETS.append(target.argmax(1))\n        else:\n            TARGETS.append(target)    \n\n        pred = logits.softmax(1).argmax(1).detach()\n        PREDS.append(pred)\n        IMGIDS.append(imgid)\n\n        loss_np = loss.detach().cpu().numpy()\n        train_loss.append(loss_np)\n        bar.set_description('train_loss: %.5f' % (loss_np))\n\n    PREDS = torch.cat(PREDS).cpu().numpy()\n    TARGETS = torch.cat(TARGETS).cpu().numpy()\n    IMGIDS = [i for sub in IMGIDS for i in sub]\n\n    acc = (PREDS == TARGETS).mean() * 100.\n    bleu = calculate_bleu_score(PREDS,TARGETS,idx2ans)\n\n    return np.mean(train_loss), PREDS, acc, bleu, IMGIDS","metadata":{"execution":{"iopub.status.busy":"2021-10-15T14:30:32.726958Z","iopub.execute_input":"2021-10-15T14:30:32.727893Z","iopub.status.idle":"2021-10-15T14:30:32.744867Z","shell.execute_reply.started":"2021-10-15T14:30:32.727848Z","shell.execute_reply":"2021-10-15T14:30:32.743921Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Eval methods\n","metadata":{}},{"cell_type":"code","source":"def validate(loader, model, criterion, device, scaler, args, val_df, idx2ans):\n\n    model.eval()\n    val_loss = []\n\n    PREDS = []\n    TARGETS = []\n    bar = tqdm(loader, leave=False)\n\n    with torch.no_grad():\n        for (img, question_token,segment_ids,attention_mask,target, _) in bar:\n\n            img, question_token,segment_ids,attention_mask,target = img.to(device), question_token.to(device), segment_ids.to(device), attention_mask.to(device), target.to(device)\n            question_token = question_token.squeeze(1)\n            attention_mask = attention_mask.squeeze(1)\n\n\n            if args.mixed_precision:\n                with torch.cuda.amp.autocast(): \n                    logits, _, _ = model(img, question_token, segment_ids, attention_mask)\n                    loss = criterion(logits, target)\n            else:\n                logits, _ , _= model(img, question_token, segment_ids, attention_mask)\n                loss = criterion(logits, target)\n\n\n            loss_np = loss.detach().cpu().numpy()\n\n            pred = logits.softmax(1).argmax(1).detach()\n\n            PREDS.append(pred)\n\n            if args.smoothing:\n                TARGETS.append(target.argmax(1))\n            else:\n                TARGETS.append(target)\n\n            val_loss.append(loss_np)\n\n            bar.set_description('val_loss: %.5f' % (loss_np))\n\n        val_loss = np.mean(val_loss)\n\n    PREDS = torch.cat(PREDS).cpu().numpy()\n    TARGETS = torch.cat(TARGETS).cpu().numpy()\n\n    # Calculate total and category wise accuracy\n    if args.category:\n        acc = (PREDS == TARGETS).mean() * 100.\n        bleu = calculate_bleu_score(PREDS,TARGETS,idx2ans)\n    else:\n        total_acc = (PREDS == TARGETS).mean() * 100.\n        plane_acc = (PREDS[val_df['category']=='plane'] == TARGETS[val_df['category']=='plane']).mean() * 100.\n        organ_acc = (PREDS[val_df['category']=='organ'] == TARGETS[val_df['category']=='organ']).mean() * 100.\n        modality_acc = (PREDS[val_df['category']=='modality'] == TARGETS[val_df['category']=='modality']).mean() * 100.\n        abnorm_acc = (PREDS[val_df['category']=='abnormality'] == TARGETS[val_df['category']=='abnormality']).mean() * 100.\n        \"\"\"plane_acc = (PREDS['plane' in val_df['category']] == TARGETS['plane' in val_df['category']]).mean() * 100.\n        organ_acc = (PREDS['organ' in val_df['category']] == TARGETS['organ' in val_df['category']]).mean() * 100.\n        modality_acc = (PREDS['modality' in val_df['category']] == TARGETS['modality' in val_df['category']]).mean() * 100.\n        abnorm_acc = (PREDS['abnormality' in val_df['category']] == TARGETS['abnormality' in val_df['category']]).mean() * 100.\"\"\"\n\n        acc = {'val_total_acc': np.round(total_acc, 4), 'val_plane_acc': np.round(plane_acc, 4), 'val_organ_acc': np.round(organ_acc, 4), \n               'val_modality_acc': np.round(modality_acc, 4), 'val_abnorm_acc': np.round(abnorm_acc, 4)}\n\n        # add bleu score code\n        total_bleu = calculate_bleu_score(PREDS,TARGETS,idx2ans)\n        plane_bleu = calculate_bleu_score(PREDS[val_df['category']=='plane'],TARGETS[val_df['category']=='plane'],idx2ans)\n        organ_bleu = calculate_bleu_score(PREDS[val_df['category']=='organ'],TARGETS[val_df['category']=='organ'],idx2ans)\n        modality_bleu = calculate_bleu_score(PREDS[val_df['category']=='modality'],TARGETS[val_df['category']=='modality'],idx2ans)\n        abnorm_bleu = calculate_bleu_score(PREDS[val_df['category']=='abnormality'],TARGETS[val_df['category']=='abnormality'],idx2ans)\n        \"\"\"plane_bleu = calculate_bleu_score(PREDS['plane' in val_df['category']],TARGETS['plane' in val_df['category']],idx2ans)\n        organ_bleu = calculate_bleu_score(PREDS['organ' in val_df['category']],TARGETS['organ' in val_df['category']],idx2ans)\n        modality_bleu = calculate_bleu_score(PREDS['modality' in val_df['category']],TARGETS['modality' in val_df['category']],idx2ans)\n        abnorm_bleu = calculate_bleu_score(PREDS['abnormality' in val_df['category']],TARGETS['abnormality' in val_df['category']],idx2ans)\"\"\"\n\n\n        bleu = {'val_total_bleu': np.round(total_bleu, 4), 'val_plane_bleu': np.round(plane_bleu, 4), 'val_organ_bleu': np.round(organ_bleu, 4), \n            'val_modality_bleu': np.round(modality_bleu, 4), 'val_abnorm_bleu': np.round(abnorm_bleu, 4)}\n\n    return val_loss, PREDS, acc, bleu  ","metadata":{"execution":{"iopub.status.busy":"2021-10-15T14:30:32.746511Z","iopub.execute_input":"2021-10-15T14:30:32.746863Z","iopub.status.idle":"2021-10-15T14:30:32.770019Z","shell.execute_reply.started":"2021-10-15T14:30:32.746837Z","shell.execute_reply":"2021-10-15T14:30:32.769392Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def test(loader, model, criterion, device, scaler, args, val_df,idx2ans):\n\n    model.eval()\n    TARGETS = []\n    PREDS = []\n    test_loss = []\n\n    with torch.no_grad():\n        for (img,question_token,segment_ids,attention_mask,target, _) in tqdm(loader, leave=False):\n\n            img, question_token, segment_ids, attention_mask, target = img.to(device), question_token.to(device), segment_ids.to(device), attention_mask.to(device), target.to(device)\n            question_token = question_token.squeeze(1)\n            attention_mask = attention_mask.squeeze(1)\n            \n            if args.mixed_precision:\n                with torch.cuda.amp.autocast(): \n                    logits, _, _ = model(img, question_token, segment_ids, attention_mask)\n                    loss = criterion(logits, target)\n            else:\n                logits, _, _ = model(img, question_token, segment_ids, attention_mask)\n                loss = criterion(logits, target)\n\n\n            loss_np = loss.detach().cpu().numpy()\n\n            test_loss.append(loss_np)\n\n            pred = logits.softmax(1).argmax(1).detach()\n            \n            PREDS.append(pred)\n\n            if args.smoothing:\n                TARGETS.append(target.argmax(1))\n            else:\n                TARGETS.append(target)\n\n        test_loss = np.mean(test_loss)\n\n    PREDS = torch.cat(PREDS).cpu().numpy()\n    TARGETS = torch.cat(TARGETS).cpu().numpy()\n\n    if args.category:\n        acc = (PREDS == TARGETS).mean() * 100.\n        bleu = calculate_bleu_score(PREDS,TARGETS,idx2ans)\n    else:\n        total_acc = (PREDS == TARGETS).mean() * 100.\n        plane_acc = (PREDS[val_df['category']=='plane'] == TARGETS[val_df['category']=='plane']).mean() * 100.\n        organ_acc = (PREDS[val_df['category']=='organ'] == TARGETS[val_df['category']=='organ']).mean() * 100.\n        modality_acc = (PREDS[val_df['category']=='modality'] == TARGETS[val_df['category']=='modality']).mean() * 100.\n        abnorm_acc = (PREDS[val_df['category']=='abnormality'] == TARGETS[val_df['category']=='abnormality']).mean() * 100.\n        \"\"\"plane_acc = (PREDS['plane' in val_df['category']] == TARGETS['plane' in val_df['category']]).mean() * 100.\n        organ_acc = (PREDS['organ' in val_df['category']] == TARGETS['organ' in val_df['category']]).mean() * 100.\n        modality_acc = (PREDS['modality' in val_df['category']] == TARGETS['modality' in val_df['category']]).mean() * 100.\n        abnorm_acc = (PREDS['abnormality' in val_df['category']] == TARGETS['abnormality' in val_df['category']]).mean() * 100.\"\"\"\n\n        acc = {'total_acc': np.round(total_acc, 4), 'plane_acc': np.round(plane_acc, 4), 'organ_acc': np.round(organ_acc, 4), \n               'modality_acc': np.round(modality_acc, 4), 'abnorm_acc': np.round(abnorm_acc, 4)}\n\n        # add bleu score code\n        total_bleu = calculate_bleu_score(PREDS,TARGETS,idx2ans)\n        plane_bleu = calculate_bleu_score(PREDS[val_df['category']=='plane'],TARGETS[val_df['category']=='plane'],idx2ans)\n        organ_bleu = calculate_bleu_score(PREDS[val_df['category']=='organ'],TARGETS[val_df['category']=='organ'],idx2ans)\n        modality_bleu = calculate_bleu_score(PREDS[val_df['category']=='modality'],TARGETS[val_df['category']=='modality'],idx2ans)\n        abnorm_bleu = calculate_bleu_score(PREDS[val_df['category']=='abnormality'],TARGETS[val_df['category']=='abnormality'],idx2ans)\n        \"\"\"plane_bleu = calculate_bleu_score(PREDS['plane' in val_df['category']],TARGETS['plane' in val_df['category']],idx2ans)\n        organ_bleu = calculate_bleu_score(PREDS['organ' in val_df['category']],TARGETS['organ' in val_df['category']],idx2ans)\n        modality_bleu = calculate_bleu_score(PREDS['modality' in val_df['category']],TARGETS['modality' in val_df['category']],idx2ans)\n        abnorm_bleu = calculate_bleu_score(PREDS['abnormality' in val_df['category']],TARGETS['abnormality' in val_df['category']],idx2ans)\"\"\"\n\n\n        bleu = {'total_bleu': np.round(total_bleu, 4), 'plane_bleu': np.round(plane_bleu, 4), 'organ_bleu': np.round(organ_bleu, 4), \n            'modality_bleu': np.round(modality_bleu, 4), 'abnorm_bleu': np.round(abnorm_bleu, 4)}\n\n\n    return test_loss, PREDS, acc, bleu","metadata":{"execution":{"iopub.status.busy":"2021-10-15T14:30:32.771871Z","iopub.execute_input":"2021-10-15T14:30:32.772218Z","iopub.status.idle":"2021-10-15T14:30:32.796339Z","shell.execute_reply.started":"2021-10-15T14:30:32.772153Z","shell.execute_reply":"2021-10-15T14:30:32.795484Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Train","metadata":{}},{"cell_type":"code","source":"import argparse\nimport sys\n#from utils import seed_everything, Model, VQAMed, train_one_epoch, validate, test, load_data, LabelSmoothing, train_img_only, val_img_only, test_img_only\nimport wandb\nimport pandas as pd\nimport numpy as np\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader, Dataset\nimport torch.optim as optim\nimport torch.optim.lr_scheduler as lr_scheduler\nfrom torchvision import transforms, models\nfrom torch.cuda.amp import GradScaler\nimport os\nimport warnings\nimport albumentations as A\nimport pretrainedmodels\nfrom albumentations.core.composition import OneOf\n#from albumentations.pytorch.transforms import ToTensorV2\n\nwarnings.simplefilter(\"ignore\", UserWarning)","metadata":{"execution":{"iopub.status.busy":"2021-10-15T14:30:32.797434Z","iopub.execute_input":"2021-10-15T14:30:32.797649Z","iopub.status.idle":"2021-10-15T14:30:34.698058Z","shell.execute_reply.started":"2021-10-15T14:30:32.797612Z","shell.execute_reply":"2021-10-15T14:30:34.696775Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#os.mkdir('/kaggle/working/roco-weights')\n\nsys.argv = ['-f']\n\nparser = argparse.ArgumentParser(description = \"Finetune on ImageClef 2019\")\n\nparser.add_argument('--run_name', type = str, required = False, default = \"MMBERT_allclef_vqarad_pre\", help = \"run name for wandb\")\n#parser.add_argument('--clef_data_dir', type = str, required = False, default = \"../input/allclef-reply\", help = \"path for clef data\")\n#parser.add_argument('--vqarad_data_dir', type = str, required = False, default = \"../input/vqarad-reply\", help = \"path for vqarad data\")\nparser.add_argument('--model_dir', type = str, required = False, default = \"../input/mmbert-allclef-vqarad-pre-weights/MMBERT_allclef_vqarad_pre_bestacc.pt\", help = \"path to load weights\")\n#parser.add_argument('--model_dir', type = str, required = False, default = \"../input/mmbert-pretrain-roco-weights/rocopretrain_weights.pt\", help = \"path to load weights\")\n#parser.add_argument('--save_dir', type = str, required = False, default = \"/content/drive/MyDrive/Colab Notebooks/Thesis/Transformer VQA/MMBERT weights/MMBERTallClef_noPre\", help = \"path to save weights\")\nparser.add_argument('--category', type = str, required = False, default = None,  help = \"choose specific category if you want\")\nparser.add_argument('--use_pretrained', action = 'store_true', default = False, help = \"use pretrained weights or not\")\nparser.add_argument('--mixed_precision', action = 'store_true', default = False, help = \"use mixed precision or not\")\nparser.add_argument('--clip', action = 'store_true', default = False, help = \"clip the gradients or not\")\n\nparser.add_argument('--seed', type = int, required = False, default = 42, help = \"set seed for reproducibility\")\nparser.add_argument('--num_workers', type = int, required = False, default = 4, help = \"number of workers\")\nparser.add_argument('--epochs', type = int, required = False, default = 100, help = \"num epochs to train\")\nparser.add_argument('--train_pct', type = float, required = False, default = 1.0, help = \"fraction of train samples to select\")\nparser.add_argument('--valid_pct', type = float, required = False, default = 1.0, help = \"fraction of validation samples to select\")\nparser.add_argument('--test_pct', type = float, required = False, default = 1.0, help = \"fraction of test samples to select\")\n\nparser.add_argument('--max_position_embeddings', type = int, required = False, default = 28, help = \"max length of sequence\")\nparser.add_argument('--batch_size', type = int, required = False, default = 10, help = \"batch size\")\nparser.add_argument('--lr', type = float, required = False, default = 1e-4, help = \"learning rate'\")\n# parser.add_argument('--weight_decay', type = float, required = False, default = 1e-2, help = \" weight decay for gradients\")\nparser.add_argument('--factor', type = float, required = False, default = 0.1, help = \"factor for rlp\")\nparser.add_argument('--patience', type = int, required = False, default = 10, help = \"patience for rlp\")\n# parser.add_argument('--lr_min', type = float, required = False, default = 1e-6, help = \"minimum lr for Cosine Annealing\")\nparser.add_argument('--hidden_dropout_prob', type = float, required = False, default = 0.3, help = \"hidden dropout probability\")\nparser.add_argument('--smoothing', type = float, required = False, default = None, help = \"label smoothing\")\n\nparser.add_argument('--image_size', type = int, required = False, default = 224, help = \"image size\")\nparser.add_argument('--hidden_size', type = int, required = False, default = 768, help = \"hidden size\") #og 312\nparser.add_argument('--vocab_size', type = int, required = False, default = 30522, help = \"vocab size\")\nparser.add_argument('--type_vocab_size', type = int, required = False, default = 2, help = \"type vocab size\")\nparser.add_argument('--heads', type = int, required = False, default = 12, help = \"heads\")\nparser.add_argument('--n_layers', type = int, required = False, default = 4, help = \"num of layers\")\nparser.add_argument('--num_vis', type = int, required = False , default = 5, help = \"num of visual embeddings\") #num of conv2d Layers in the transformer, can be: 5, 3 or 1\n\nargs = parser.parse_args()\n\n\"\"\"wandb.init(project='medvqa', name = args.run_name, config = args)\"\"\"\n\nseed_everything(args.seed)","metadata":{"execution":{"iopub.status.busy":"2021-10-15T14:30:34.700762Z","iopub.execute_input":"2021-10-15T14:30:34.701015Z","iopub.status.idle":"2021-10-15T14:30:34.727951Z","shell.execute_reply.started":"2021-10-15T14:30:34.700985Z","shell.execute_reply":"2021-10-15T14:30:34.726856Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pd.set_option('display.max_columns', None)\npd.set_option('display.width', None)\npd.set_option('display.max_colwidth', None)","metadata":{"execution":{"iopub.status.busy":"2021-10-15T14:30:34.729255Z","iopub.execute_input":"2021-10-15T14:30:34.729989Z","iopub.status.idle":"2021-10-15T14:30:34.738921Z","shell.execute_reply.started":"2021-10-15T14:30:34.729955Z","shell.execute_reply":"2021-10-15T14:30:34.738039Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df, val_df, test_df = load_all_data(args)\nprint('len(train_df): ' ,len(train_df))\nprint('len(val_df): ' ,len(val_df))\nprint('len(test_df): ' ,len(test_df))\n\n# remove abn\ntrain_df = train_df.loc[train_df['category'].str.contains('abnormality')]\nval_df = val_df.loc[val_df['category'].str.contains('abnormality')]\ntest_df = test_df.loc[test_df['category'].str.contains('abnormality')]\n\nprint('len(train_df): ' ,len(train_df))\nprint('len(val_df): ' ,len(val_df))\nprint('len(test_df): ' ,len(test_df))\n\nif args.category:\n        \n    train_df = train_df[train_df['category']==args.category].reset_index(drop=True)\n    val_df = val_df[val_df['category']==args.category].reset_index(drop=True)\n    test_df = test_df[test_df['category']==args.category].reset_index(drop=True)\n\n    train_df = train_df[~train_df['answer'].isin(['yes', 'no'])].reset_index(drop = True)\n    val_df = val_df[~val_df['answer'].isin(['yes', 'no'])].reset_index(drop = True)\n    test_df = test_df[~test_df['answer'].isin(['yes', 'no'])].reset_index(drop = True)\n\ndf = pd.concat([train_df, val_df, test_df]).reset_index(drop=True)\n\nans2idx = {ans:idx for idx,ans in enumerate(df['answer'].unique())}\nidx2ans = {idx:ans for ans,idx in ans2idx.items()}\ndf['answer_mapped'] = df['answer'].map(ans2idx).astype(int)\ntrain_df = df[df['mode']=='train'].reset_index(drop=True)\nval_df = df[df['mode']=='val'].reset_index(drop=True)\ntest_df = df[df['mode']=='test'].reset_index(drop=True)\n\nnum_classes = len(ans2idx)\n\nargs.num_classes = num_classes\nprint(num_classes)\n\ndf.head(50)\n","metadata":{"execution":{"iopub.status.busy":"2021-10-15T14:32:15.990776Z","iopub.execute_input":"2021-10-15T14:32:15.991636Z","iopub.status.idle":"2021-10-15T14:32:16.34721Z","shell.execute_reply.started":"2021-10-15T14:32:15.991578Z","shell.execute_reply":"2021-10-15T14:32:16.346189Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"device = 'cuda' if torch.cuda.is_available() else 'cpu'\n\nmodel = Model(args)","metadata":{"execution":{"iopub.status.busy":"2021-10-10T21:21:19.885784Z","iopub.execute_input":"2021-10-10T21:21:19.886179Z","iopub.status.idle":"2021-10-10T21:21:39.362963Z","shell.execute_reply.started":"2021-10-10T21:21:19.886144Z","shell.execute_reply":"2021-10-10T21:21:39.361884Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.classifier[2] = nn.Linear(args.hidden_size, num_classes)\n\nif args.use_pretrained:\n    print(\"loading weights\")\n    model.load_state_dict(torch.load(args.model_dir))\n    print(\"loaded weights\")\n \nmodel.to(device)","metadata":{"execution":{"iopub.status.busy":"2021-10-10T21:21:39.366955Z","iopub.execute_input":"2021-10-10T21:21:39.367179Z","iopub.status.idle":"2021-10-10T21:21:51.759235Z","shell.execute_reply.started":"2021-10-10T21:21:39.367153Z","shell.execute_reply":"2021-10-10T21:21:51.757489Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"optimizer = optim.Adam(model.parameters(),lr=args.lr)\nscheduler = lr_scheduler.ReduceLROnPlateau(optimizer, patience = args.patience, factor = args.factor, verbose = True)\n\nif args.smoothing:\n    criterion = LabelSmoothing(smoothing=args.smoothing)\nelse:\n    criterion = nn.CrossEntropyLoss()\n\nscaler = GradScaler()\n\n\ntrain_tfm = transforms.Compose([transforms.ToPILImage(),\n                                transforms.RandomResizedCrop(224,scale=(0.75,1.25),ratio=(0.75,1.25)),\n                                transforms.RandomRotation(10),\n                                # Cutout(),\n                                transforms.ColorJitter(brightness=0.4,contrast=0.4,saturation=0.4,hue=0.4),\n                                transforms.ToTensor(), \n                                transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n\nval_tfm = transforms.Compose([transforms.ToPILImage(),\n                              transforms.Resize((224,224)),\n                              transforms.ToTensor(), \n                              transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n\ntest_tfm = transforms.Compose([transforms.ToPILImage(),\n                               transforms.Resize((224,224)),    \n                               transforms.ToTensor(), \n                               transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])","metadata":{"execution":{"iopub.status.busy":"2021-10-10T21:21:51.760036Z","iopub.status.idle":"2021-10-10T21:21:51.760353Z","shell.execute_reply.started":"2021-10-10T21:21:51.76019Z","shell.execute_reply":"2021-10-10T21:21:51.760208Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"traindataset = VQAMed(train_df, imgsize = args.image_size, tfm = train_tfm, args = args)\nvaldataset = VQAMed(val_df, imgsize = args.image_size, tfm = val_tfm, args = args)\ntestdataset = VQAMed(test_df, imgsize = args.image_size, tfm = test_tfm, args = args)\n\ntrainloader = DataLoader(traindataset, batch_size = args.batch_size, shuffle=True, num_workers = args.num_workers)\nvalloader = DataLoader(valdataset, batch_size = args.batch_size, shuffle=False, num_workers = args.num_workers)\ntestloader = DataLoader(testdataset, batch_size = args.batch_size, shuffle=False, num_workers = args.num_workers)","metadata":{"execution":{"iopub.status.busy":"2021-10-10T21:21:51.762018Z","iopub.status.idle":"2021-10-10T21:21:51.762831Z","shell.execute_reply.started":"2021-10-10T21:21:51.762582Z","shell.execute_reply":"2021-10-10T21:21:51.762606Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from datetime import datetime\nnow = datetime.now()\n\nwith open(\"../input/mmbert-allclef-vqarad-pre-weights/MMBERT_allclef_vqarad_pre.txt\") as input_txt:\n    with open(\"MMBERT_allclef_vqarad_pre.txt\", \"w\") as f:\n        for line in input_txt:\n            f.write(line) \n\nf = open(f'{args.run_name}.txt', \"a\")\n#f.write(\"datasets used: clef2018, clef2019, clef2020, vqa-rad\")\n#f.write(\"pretrain on ROCO\")\nf.write(\"\\n\\n\\nMMBERT training \" + str(now))\nf.close()","metadata":{"execution":{"iopub.status.busy":"2021-10-10T21:21:51.763892Z","iopub.status.idle":"2021-10-10T21:21:51.764681Z","shell.execute_reply.started":"2021-10-10T21:21:51.764446Z","shell.execute_reply":"2021-10-10T21:21:51.76447Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#best_acc = 0\n#best_loss = np.inf\ncounter = 0\n\nbest_acc = 52.9058\nbest_loss = 2.9077919","metadata":{"execution":{"iopub.status.busy":"2021-10-10T21:21:51.765723Z","iopub.status.idle":"2021-10-10T21:21:51.766304Z","shell.execute_reply.started":"2021-10-10T21:21:51.766033Z","shell.execute_reply":"2021-10-10T21:21:51.766055Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for epoch in range(args.epochs):\n\n    print(f'Epoch {epoch+1}/{args.epochs}')\n\n\n    train_loss, _, train_acc, _, _ = train_one_epoch(trainloader, model, optimizer, criterion, device, scaler, args, idx2ans)\n    val_loss, val_predictions, val_acc, val_bleu = validate(valloader, model, criterion, device, scaler, args, val_df,idx2ans)\n    test_loss, test_predictions, test_acc, test_bleu = test(testloader, model, criterion, device, scaler, args, test_df,idx2ans)\n\n    scheduler.step(val_loss)\n\n    print(\"val_loss: \" ,val_loss)\n    print(\"val_acc: \" ,val_acc)\n    print(\"test_acc: \" ,test_acc)\n    \n    f = open(f'{args.run_name}.txt', \"a\")\n    f.write('\\n\\nepoch ' + str(epoch))\n    f.write('\\nAccuracy and Loss')\n    f.write('\\ntrain_acc: ' + str(train_acc) + '   train_loss: ' + str(train_loss) + ',')\n    f.write('\\nval_acc: ' + str(val_acc) + '   val_loss: ' + str(val_loss) + ',')\n    f.write('\\ntest_acc: ' + str(test_acc) + '   test_loss: ' + str(test_loss) + ',')\n    f.write('\\nBLEU validation: ' + str(val_bleu))\n    f.write('\\nBLEU test: ' + str(test_bleu))\n    f.write('\\nlearning_rate: ' + str(optimizer.param_groups[0][\"lr\"]))\n\n    if test_acc['total_acc'] > best_acc:\n        print('Saving model best acc')\n        f.write('\\nnew best test total acc')\n        torch.save(model.state_dict(), f'{args.run_name}_bestacc.pt')\n        best_acc=test_acc['total_acc']\n    \n    if val_loss < best_loss:\n        print('Saving model best val loss')\n        f.write('\\nnew best val_loss')\n        torch.save(model.state_dict(), f'{args.run_name}.pt')\n        best_loss=val_loss\n        counter=0\n        f.write('\\ncounter: ' + str(counter))\n    else:\n        counter+=1\n        print(\"counter: \" ,counter)\n        f.write('\\ncounter: ' + str(counter))\n        if counter > 20:\n            break\n    \n    torch.save(model.state_dict(), f'{args.run_name}_lastepoch.pt')\n    f.close()","metadata":{"execution":{"iopub.status.busy":"2021-10-10T21:21:51.767705Z","iopub.status.idle":"2021-10-10T21:21:51.768254Z","shell.execute_reply.started":"2021-10-10T21:21:51.768022Z","shell.execute_reply":"2021-10-10T21:21:51.768044Z"},"trusted":true},"execution_count":null,"outputs":[]}]}